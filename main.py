# -*- coding: utf-8 -*-
"""Ask-Paul-AI-Chatbot-with-custom-knowledge-base

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Ag6PWX_45_cg772rwWqwSveWGh5WvS5

#Introduction

This notebook has all the code you need to create your own chatbot with custom knowledge base using GPT-3. It will answer questions about a corpus of text and run on Colab.

Follow the instructions for each steps and then run the code sample. In order to run the code, you need to press "play" button near each code sample.


Important Note: This is a basic chatbot running on a limited selection of articles from Paul Graham newsletter. It's simply a starting point.

For questions, feel free to reach out on [Twitter](https://twitter.com/desireyavro).

#Download the data for your custom knowledge base
For the demonstration purposes we are going to use ----- as our knowledge base. You can download them to your local folder from the github repository by running the code below.
Alternatively, you can put your own custom data into the local folder.
"""

! git clone https://github.com/dnzengou/yc-startup-playbook.git

"""# Install the dependicies
Run the code below to install the depencies we need for our functions
"""

!pip install llama-index==0.5.6
!pip install langchain==0.0.148

"""# Define the functions
The following code defines the functions we need to construct the index and query it
"""

from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext
from langchain import OpenAI
import sys
import os
from IPython.display import Markdown, display

def construct_index(directory_path):
    # set maximum input size
    max_input_size = 4096
    # set number of output tokens
    num_outputs = 2000
    # set maximum chunk overlap
    max_chunk_overlap = 20
    # set chunk size limit
    chunk_size_limit = 600

    # define prompt helper
    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    # define LLM
    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name="text-davinci-003", max_tokens=num_outputs))

    documents = SimpleDirectoryReader(directory_path).load_data()

    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)
    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)

    index.save_to_disk('index.json')

    return index

def ask_paul():
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    while True:
        query = input("What do you want to know? ")
        response = index.query(query)
        display(Markdown(f"Response: <b>{response.response}</b>"))

"""# Set OpenAI API Key
You need an OPENAI API key to be able to run this code.

If you don't have one yet, get it by [signing up](https://platform.openai.com/overview). Then click your account icon on the top right of the screen and select "View API Keys". Create an API key.

Then run the code below and paste your API key into the text input.
"""

os.environ["OPENAI_API_KEY"] = input("Paste your OpenAI key here and hit enter:")

"""#Construct an index
Now we are ready to construct the index. This will take every file in the folder 'data', split it into chunks, and embed it with OpenAI's embeddings API.

**Notice:** running this code will cost you credits on your OpenAPI account ($0.02 for every 1,000 tokens). If you've just set up your account, the free credits that you have should be more than enough for this experiment.
"""

construct_index('/content/yc-startup-playbook/ask-yc-paul-graham')

"""#Ask questions
It's time to have fun and test the AI chatbot. Run the function that queries GPT and type your question into the input.

If you've used the provided example data for your custom knowledge base, here are a few questions that you can ask:
1. Who is Paul Graham?
2. What is YC?
3. How to grow a startup?
4. Where to raise funds?
5. If that is what you want, how to get rich now?
"""

ask_paul()

